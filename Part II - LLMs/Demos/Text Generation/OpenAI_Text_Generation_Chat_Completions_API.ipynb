{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vlad-Enia/NN-LLM-Intro/blob/master/Part%20II%20-%20LLMs/Demos/Text%20Generation/OpenAI_Text_Generation_Chat_Completions_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OpenAI Account Setup\n"
      ],
      "metadata": {
        "id": "9WCNWXoo4y9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating OpenAI API key\n",
        "* First, [create](https://platform.openai.com/signup) an OpenAI account\n",
        "* Add [credit balance](https://platform.openai.com/settings/organization/billing/overview)\n",
        "  * Using the API is charged per token, each model having different prices\n",
        "  * We will mainly be using GPT-4o mini, which is the cost-effective model offered by OpenAI.\n",
        "  * For the purpose of these tutorials, $5 should be more than enough\n",
        "* Generate an [API key](https://platform.openai.com/api-keys)"
      ],
      "metadata": {
        "id": "E-Yf0KWf64qx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Storing and using the API key\n",
        "* Store the key as a secret in Google Colab\n",
        "  * Click the \"Key\" button on the left toolbar\n",
        "  * Add the API key as a secret with name `OPENAI_API_KEY`\n",
        "  * Enable `Notebook access` for the secret\n",
        "  * Access the key using:"
      ],
      "metadata": {
        "id": "mz8ZCDyE6guW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "from openai import OpenAI\n",
        "\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)"
      ],
      "metadata": {
        "id": "ih8q9PPo-ClM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqaJxQ0r3VE8"
      },
      "source": [
        "# Generating text with Large Language Models (LLMs)\n",
        "\n",
        "`GPT-4o mini` is one a cost-effiecient model by [OpenAI](https://openai.com/). \" `GPT-4o Mini` can generate human-like prose by responding to prompts written in the [Chat Markup Language](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt), or ChatML for short. Here are few examples demonstrating how to leverage `GPT-4o mini`'s text-generation capabilities. Start by asking `GPT-4o mini` why is the sky blue. Run this cell several times and you'll get a different result each time. Set `temperature` to 0, however, and the results will be the same most of the time:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9l4rTRYF3VE9",
        "outputId": "2b3103f7-f7a6-4105-eaf1-dcd2f46d3610",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sky appears blue due to a phenomenon known as Rayleigh scattering. This occurs when sunlight interacts with the Earth's atmosphere.\n",
            "\n",
            "Sunlight, though it appears white, is composed of many colors, each with different wavelengths. Blue light has a shorter wavelength compared to other colors like red or yellow. When sunlight enters the atmosphere, it collides with air molecules and small particles, scattering the shorter wavelengths of light—especially blue—more than the longer wavelengths.\n",
            "\n",
            "As a result, when you look up during the day, you see more of the scattered blue light coming from all directions, which gives the sky its characteristic blue color. During sunrise and sunset, the sun is lower in the sky, and its light passes through a thicker layer of the atmosphere. This causes more scattering of the shorter wavelengths and allows the longer wavelengths, such as red and orange, to become more prominent, resulting in beautiful colorful skies at those times.\n"
          ]
        }
      ],
      "source": [
        "messages = [{\n",
        "    'role': 'user',\n",
        "    'content': 'Why is the sky blue?'\n",
        "}]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NDr9ckH3VE9"
      },
      "source": [
        "You can richen the UI experience by streaming the response. Here's how:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "BCnijYoM3VE9",
        "outputId": "dacf4512-4257-4258-bb25-cc21fda6a68c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sky appears blue due to a phenomenon known as Rayleigh scattering. This occurs when sunlight enters the Earth's atmosphere and interacts with air molecules and small particles.\n",
            "\n",
            "Sunlight, or white light, is made up of various colors, each with different wavelengths. Blue light has a shorter wavelength compared to other colors, such as red or yellow. When sunlight passes through the atmosphere, the shorter wavelengths—like blue—are scattered in all directions by the small molecules in the air. Because blue light is scattered more efficiently than other colors, our eyes perceive the sky as blue during the day.\n",
            "\n",
            "At sunrise and sunset, the light from the sun passes through a greater thickness of the atmosphere. Consequently, the blue and green light is scattered out of the line of sight, and longer wavelengths like red, orange, and yellow become more prominent, which is why we often see beautiful reds and oranges at those times."
          ]
        }
      ],
      "source": [
        "messages = [{\n",
        "    'role': 'user',\n",
        "    'content': 'Why is the sky blue?'\n",
        "}]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='', flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7heTNb33VE9"
      },
      "source": [
        "## Context\n",
        "\n",
        "Messages transmitted to `GPT-4o-mini` use the Chat Markup Language. ChatML exists so that the context of a conversation can be preserved across calls. To demonstrate, ask the LLM what its name is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4-aVnsG3VE-",
        "outputId": "a6972362-6ca9-4d44-9a6d-a8d6c3253cde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, Vlad! I'm an AI language model and don't have a personal name, but you can just call me Assistant. How can I help you today?"
          ]
        }
      ],
      "source": [
        "messages = [{\n",
        "    'role': 'user',\n",
        "    'content': 'My name is Vlad. What\\'s your name?' \\\n",
        "}]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='', flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eV229Fg3VE-"
      },
      "source": [
        "But now try this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7zyuNjI3VE-",
        "outputId": "800119d7-9453-4d6d-a0b1-2985424f2764",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nice to meet you, Vlad! I’m called GPT. How can I assist you today?"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\n",
        "        'role': 'system',\n",
        "        'content': 'You are a friendly assistant named GPT.'\n",
        "    },\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': 'My name is Vlad. What\\'s your name?'\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='', flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J94DgGCE3VE-"
      },
      "source": [
        "You can be as specific as you’d like with `system` messages, even saying \"If you don’t know the answer to a question, say I don’t know.\" You can also prescribe a persona. Replace \"friendly\" with \"sarcastic\" in the message from system and run the code again.  \n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "messages = [\n",
        "    {\n",
        "        'role': 'system',\n",
        "        'content': 'You are a sarcastic, sassy assistant named GPT.'\n",
        "    },\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': 'My name is Vlad. What\\'s your name?'\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='', flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B-2KuImv2-fd",
        "outputId": "f9d43318-6803-4e61-e7d9-228f640410aa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Oh, you can call me GPT, but if you prefer, I’m totally open to “The Most Amazing Assistant Ever.” Just saying! So, Vlad, what’s on your mind today?"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "ChatML's greatest power lies in persisting context from one call to the next. As an example, try this:"
      ],
      "metadata": {
        "id": "89DbLfO03Cej"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "2hyXZVkC3VE-",
        "outputId": "2d64a76b-a3b1-4079-a501-5c51f82899bb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Nice to meet you, Vlad! I'm called GPT. How can I assist you today?"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\n",
        "        'role': 'system',\n",
        "        'content': 'You are a friendly assistant named GPT.'\n",
        "    },\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': 'My name is Vlad. What\\'s your name?'\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='', flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppDNpc8S3VE-"
      },
      "source": [
        "Then follow up immediately with this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoYOaZ2u3VE-",
        "outputId": "0153877e-b72b-45e7-a1be-c4abb5ca4cf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know your name unless you tell me. How can I assist you today?"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\n",
        "        'role': 'system',\n",
        "        'content': 'You are a friendly assistant named GPT.'\n",
        "    },\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': 'What is my name?'\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='', flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vysFP6vr3VE-"
      },
      "source": [
        "The LLM will respond with something along the lines of \"I'm sorry, but I don’t have access to that information.\" But now try this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "gHXctS083VE-",
        "outputId": "618e4a2a-1170-4035-af67-c88ccfe4f07c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your name is Vlad."
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\n",
        "        'role': 'system',\n",
        "        'content': 'You are a friendly assistant named GPT.'\n",
        "    },\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': 'My name is Vlad. What\\'s your name?'\n",
        "    },\n",
        "    {\n",
        "        'role': 'assistant',\n",
        "        'content': 'Hello Vlad, my name is GPT. Nice to meet you!'\n",
        "    },\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': 'What is my name?'\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='', flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zGlnbsD3VE-"
      },
      "source": [
        "Get it? Calls to `GPT-4o mini` are stateless. If you give `GPT-4o mini` your name in one call and ask it to repeat your name in the next call, it has no clue. But with ChatML, you can provide past responses as context for the current call. You can build a conversational assistant simply by repeating the last few prompts and responses in each call to `GPT-4o mini`. The further back you go, the longer the assistant's \"memory\" will be."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cuk_lscY3VE-"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "LLMs don't work with words; they work with *tokens*. Tokenization plays an important role in Natural Language Processing. Neural networks can’t process text, at least not directly; they only process numbers. Tokenization converts words into numbers that a deep-learning model can understand. When an LLM generates a response by predicting a series of tokens, the tokenization process is reversed to convert the tokens into human-readable text.\n",
        "\n",
        "OpenAI LLMs use a form of tokenization called [Byte-Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding) (BPE).\n",
        "\n",
        "\n",
        "As a rule of thumb, 3 words on average translate to about 4 BPE tokens. That’s important because LLMs limit the number of tokens in each API call. The maximum token count is controlled by a parameter named `max_tokens`. For `GPT-4o-mini`, which has a context window size of `128K` (i.e. you can input `128000` tokens), it's enough to pass in a document that's a few hundred pages long. If the number of tokens generated exceeds `max_tokens`, then either the call will fail or the response will be truncated.\n",
        "\n",
        "You can compute the number of tokens generated from a text sample with help from a Python package named [`tiktoken`](https://pypi.org/project/tiktoken/):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-btL_Kx3E8mN",
        "outputId": "49751c97-e592-4549-d12c-fca3990c1a14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m39.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8typHnn3VE-",
        "outputId": "be031437-1dc2-4581-8794-a03da9964393",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "93 tokens\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "text = '''\n",
        "    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n",
        "    Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n",
        "    Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\n",
        "    Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
        "    '''\n",
        "\n",
        "encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
        "num_tokens = len(encoding.encode(text))\n",
        "print(f'{num_tokens} tokens')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuLy40953VE_"
      },
      "source": [
        "You can learn more about tokens and how to estimate the number of tokens in a `messages` object using [OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDzfVNH33VE_"
      },
      "source": [
        "There are a couple of reasons to be aware of the token count in each call. First, you’re charged by the token for input and output. The larger the `messages` array and the longer the response, the more you pay. Second, when using the messages array to provide context from previous calls, you have a finite amount of space to work with. It's common practice to pick a number – say, 10 or 20 – and limit the context from previous calls to that number of messages, or to programmatically compute the number of tokens that a conversation comprises and include as many messages as `max_tokens` will allow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts5z3Za33VE_"
      },
      "source": [
        "## Natural Language Processing\n",
        "\n",
        "`GPT-4o mini` can perform many NLP tasks such as sentiment analysis and neural machine translation (NMT) without further training. Here's an example that translates text from English to French. It's a good idea to set `temperature` to 0 here since you generally want translations to be accurate and repeatable rather than creative:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Neural Machine Translation"
      ],
      "metadata": {
        "id": "WCYDM2TwLvz4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "qAuSht4S3VE_",
        "outputId": "483e1847-ecf1-42fb-e3d0-70ea9f173fc5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brza smeđa lisica preskoči lenju psa."
          ]
        }
      ],
      "source": [
        "content =  f'Translate the following text from English to Serbian: The quick brown fox jumps over the lazy dog'\n",
        "\n",
        "messages = [{ 'role': 'user', 'content': content }]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    temperature=0,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='', flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sentiment Analysis"
      ],
      "metadata": {
        "id": "-JzLTQZrL0t1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIe2hTyS3VE_"
      },
      "source": [
        "The following examples demonstrate how to use `GPT-4o-mini` for sentiment analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XRIl2gHJ3VE_",
        "outputId": "6f443aaf-f35e-4a7a-9466-47a9194e1ed1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentiment of the review is positive."
          ]
        }
      ],
      "source": [
        "content = '''\n",
        "    Indicate whether the following review's sentiment is positive or\n",
        "    negative: Great food and excellent service.\n",
        "    '''\n",
        "\n",
        "messages = [{ 'role': 'user', 'content': content }]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='', flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjWT0rTk3VE_",
        "outputId": "6a72793d-ae35-4835-c5d4-6ccc55b1e46c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentiment of the review is negative."
          ]
        }
      ],
      "source": [
        "content = '''\n",
        "    Indicate whether the following review's sentiment is positive or\n",
        "    negative: Long lines and poor customer service.\n",
        "    '''\n",
        "\n",
        "messages = [{ 'role': 'user', 'content': content }]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='', flush=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Spam Filter"
      ],
      "metadata": {
        "id": "PSbevPSsL7l2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0Figuk23VE_"
      },
      "source": [
        "Sentiment analysis is a text-classification task. LLMs can classify text in other ways, too. The next two examples demonstrate how to use `GPT-4o mini` as a spam filter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6KdNoKT3VE_",
        "outputId": "c1fa2fe8-c2bd-43c1-e23d-ff7d1efa2e96",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The email is not spam. It contains a clear and relevant message regarding a code review meeting, which suggests it is likely intended for specific recipients."
          ]
        }
      ],
      "source": [
        "content = '''\n",
        "    Indicate whether the following email is spam or not spam:\n",
        "    Please plan to attend the code review at 2:00 p.m. this afternoon.\n",
        "    '''\n",
        "\n",
        "messages = [{ 'role': 'user', 'content': content }]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='', flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "8ZBgSV2y3VE_",
        "outputId": "4a19671a-3f99-4918-857f-577c1e82e286",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The email is likely spam. It promotes the online ordering of prescription medications, which is commonly associated with spam or fraudulent activity, especially when it emphasizes saving money without proper oversight."
          ]
        }
      ],
      "source": [
        "content = '''\n",
        "    Indicate whether the following email is spam or not spam:\n",
        "    Order prescription meds online and save $$$.\n",
        "    '''\n",
        "\n",
        "messages = [{ 'role': 'user', 'content': content }]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='', flush=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content = '''\n",
        "    Indicate whether the following email is spam or not spam:\n",
        "    Order prescription meds online and save $$$.\n",
        "    '''\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "      'role': 'system',\n",
        "      'content':\n",
        "        '''You are a spam filter. Please evaluate using \\\"Spam\\\" or \\\"Not Spam\".\n",
        "          If you cannot evaluate, please respond with \\\"Cannot evaluate\\\"'''\n",
        "    },\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': content\n",
        "    }\n",
        "  ]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='', flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GI8CFaIoIlXF",
        "outputId": "578201ce-74ca-4be0-ae16-d00b7e1e575f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spam"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content = '''\n",
        "    12341241241\n",
        "    '''\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "      'role': 'system',\n",
        "      'content':\n",
        "        '''You are a spam filter. Please evaluate using \\\"Spam\\\" or \\\"Not Spam\".\n",
        "          If you cannot evaluate, please respond with \\\"Cannot evaluate\\\"'''\n",
        "    },\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': content\n",
        "    }\n",
        "  ]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='', flush=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5NjKNGOJfU7",
        "outputId": "5a2b3f67-7896-4e59-ad8e-0be5753f9812"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cannot evaluate"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Parsing Raw Data"
      ],
      "metadata": {
        "id": "7-aK6nk5MBE3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJN58wlf3VE_"
      },
      "source": [
        "A practical use for LLMs is parsing freeform address fields and generating structured data. Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cd2iYo-73VE_",
        "outputId": "7ff0e043-f436-499a-96f1-b0948797ef83",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"Name\": \"Recipient\",\n",
            "    \"Street Address\": \"11 Aviation Avenue\",\n",
            "    \"City\": \"Charlottetown\",\n",
            "    \"State\": \"Prince Edward Island\",\n",
            "    \"Country\": \"Canada\",\n",
            "    \"Postal Code\": \"C1E 0A1\"\n",
            "}\n",
            "{\n",
            "    \"Name\": \"Roche Molecular Systems, Inc.\",\n",
            "    \"Street Address\": \"4300 Hacienda Drive\",\n",
            "    \"City\": \"Pleasanton\",\n",
            "    \"State\": \"California\",\n",
            "    \"Country\": \"United States\",\n",
            "    \"Postal Code\": \"94588\"\n",
            "}\n",
            "{\n",
            "    \"Name\": \"Cross Research S.A.\",\n",
            "    \"Street Address\": \"Via F.A. Giorgioli 14\",\n",
            "    \"City\": \"Arzo\",\n",
            "    \"State\": \"\",\n",
            "    \"Country\": \"Switzerland\",\n",
            "    \"Postal Code\": \"6864\"\n",
            "}\n",
            "{\n",
            "    \"Name\": \"Wasdell Group, Wasdell Packaging Ltd\",\n",
            "    \"Street Address\": \"Unit 1-8, Euroway Industrial Estate, Blagrove\",\n",
            "    \"City\": \"Swindon\",\n",
            "    \"State\": \"\",\n",
            "    \"Country\": \"United Kingdom\",\n",
            "    \"Postal Code\": \"SN5 8YW\"\n",
            "}\n",
            "{\n",
            "    \"Name\": \"Policlinico Gemelli\",\n",
            "    \"Street Address\": \"Largo Gemelli 8, 4th Floor, Wing J\",\n",
            "    \"City\": \"Rome\",\n",
            "    \"State\": \"\",\n",
            "    \"Country\": \"Italy\",\n",
            "    \"Postal Code\": \"00168\"\n",
            "}\n",
            "{\n",
            "    \"Name\": \"Academisch Ziekenhuis Maastricht\",\n",
            "    \"Street Address\": \"P. Debyelaan 25 5e\",\n",
            "    \"City\": \"Maastricht\",\n",
            "    \"State\": \"Limburg\",\n",
            "    \"Country\": \"Netherlands\",\n",
            "    \"Postal Code\": \"6229 HX\"\n",
            "}\n",
            "{\n",
            "    \"Name\": \"Wintellect Brussels\",\n",
            "    \"Street Address\": \"Leuvensesteenweg 555\",\n",
            "    \"City\": \"Marken\",\n",
            "    \"State\": \"\",\n",
            "    \"Country\": \"Belgium\",\n",
            "    \"Postal Code\": \"1930\"\n",
            "}\n",
            "{\n",
            "    \"Name\": \"Recipient\",\n",
            "    \"Street Address\": \"SCM department, AstraZeneca K.K., Maihara Factory, 215-31, 215-31\",\n",
            "    \"City\": \"Miyos\",\n",
            "    \"State\": \"Shiga\",\n",
            "    \"Country\": \"Japan\",\n",
            "    \"Postal Code\": \"215-31\"\n",
            "}\n",
            "{\n",
            "    \"Name\": \"Healthcare Logistics Australia\",\n",
            "    \"Street Address\": \"7 Dolerite Way\",\n",
            "    \"City\": \"Pemulwuy\",\n",
            "    \"State\": \"New South Wales\",\n",
            "    \"Country\": \"Australia\",\n",
            "    \"Postal Code\": \"2145\"\n",
            "}\n",
            "{\n",
            "    \"Name\": \"Suncoast Research\",\n",
            "    \"Street Address\": \"2128 W Flagler St, Suite 101\",\n",
            "    \"City\": \"Miami\",\n",
            "    \"State\": \"Florida\",\n",
            "    \"Country\": \"United States\",\n",
            "    \"Postal Code\": \"33135\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "addresses = [\n",
        "    '11 Aviation Avenue, Charlottetown, PE C1E 0A1, Canada',\n",
        "    'Roche Molecular Systems, Inc., 4300 Hacienda Drive, Pleasanton, CA 94588, US',\n",
        "    'Cross Research S.A., Phase I Unit, Via F.A. Giorgioli 14, Arzo, 6864, CH',\n",
        "    'Wasdell Group, Wasdell Packaging Ltd Unit 1-8, Euroway Industrial Estate, Blagrove, Swindon, SN5 8YW, GB',\n",
        "    'Policlinico Gemelli, 4th Floor, Wing J, Largo Gemelli 8, Rome, 00168, IT',\n",
        "    'Academisch Ziekenhuis Maastricht, CDL Stamcellaboratorium, P. Debyelaan 25 5e, Maastricht, 6229 HX, NL',\n",
        "    'Wintellect Brussels, Leuvensesteenweg 555, Marken Benelux, 1930, BE',\n",
        "    'SCM department, AstraZeneca K.K., Maihara Factory, AstraZeneca K.K., 215-31, Miyos, Shiga-Ken, 215-31, JP',\n",
        "    'Healthcare Logistics Australia, 7 Dolerite Way, Pemulwuy NSW 2145, AU',\n",
        "    'Suncoast Research, 2128 W Flagler St, Suite 101, Mami, FL, 33135, US'\n",
        "]\n",
        "\n",
        "for address in addresses:\n",
        "    content = f'''\n",
        "        Parse the freeform address below into fields and return a JSON\n",
        "        representation that uses the following format. Convert country\n",
        "        abbreviations such as \"CA\" into country names such as \"Canada\"\n",
        "        and state abbreviations such as \"CA\" into state names such as\n",
        "        \"California.\" Leave unknown fields blank. Also correct any\n",
        "        obvious misspellings.\n",
        "\n",
        "        {{\n",
        "            \"Name\": \"Recipient\",\n",
        "            \"Street Address\": \"Street address\",\n",
        "            \"City\": \"City, town, etc.\",\n",
        "            \"State\": \"State, province, region, territory, canton, county, department, länder, or prefecture\",\n",
        "            \"Country\": \"Country name\",\n",
        "            \"Postal Code\": \"Postal code\"\n",
        "        }}\n",
        "\n",
        "        Address: {address}\n",
        "        '''\n",
        "\n",
        "    messages = [{ 'role': 'user', 'content': content }]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model='gpt-4o-mini',\n",
        "        messages=messages,\n",
        "        response_format={ 'type': 'json_object' },\n",
        "        stream=True\n",
        "    )\n",
        "\n",
        "    for chunk in response:\n",
        "        content = chunk.choices[0].delta.content\n",
        "        if content is not None:\n",
        "            print(content, end='', flush=True)\n",
        "\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ingesting Whole Documents"
      ],
      "metadata": {
        "id": "se27LKGNMKGe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbU0cLT-3VE_"
      },
      "source": [
        "`GPT-4o-mini`'s 128K context-window size enables it to ingest large documents for summarization or other purposes. Let's see what it can do with Microsoft's 2022 annual report. We will ask it format the output using Markdown, and then use the `IPython.display` package to properly display the generated summary."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "MSFT_ANNUAL_REPORT_FILE_PATH = userdata.get('MSFT_ANNUAL_REPORT_FILE_PATH')\n",
        "with open(MSFT_ANNUAL_REPORT_FILE_PATH, 'r') as file:\n",
        "    report = file.read()\n"
      ],
      "metadata": {
        "id": "B22wV5VcDID7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "80c81098-d1c1-4c09-e28a-c89493d5e7b1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cajZOHWN3VE_",
        "outputId": "9e80b58e-da85-4c6e-8f29-625eebed3ea8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Microsoft Annual Report Summary (Fiscal Year Ending June 30, 2022)\n\n## Overview\nIn a tumultuous year characterized by economic uncertainty, high inflation, and geopolitical challenges, Microsoft continued to thrive, experiencing record revenue of **$198 billion** and operating income of **$83 billion**. The introduction of advanced digital technologies positioned Microsoft as a leader in empowering organizations and individuals.\n\n## Mission & Responsibility\nMicrosoft's mission is to empower every person and organization on the planet to achieve more. The company emphasizes its role in driving digital transformation across industries, believing technology is crucial in overcoming contemporary challenges.\n\n### Key Examples of Impact\n- **Ferrovial**: Utilizing Microsoft cloud to build safer roads for future autonomous vehicles.\n- **Peace Parks Foundation**: Leveraging Azure AI for wildlife protection and park maintenance.\n- **Kawasaki Heavy Industries**: Developing an industrial metaverse using Azure IoT and HoloLens.\n- **Globo**: Empowering employee solutions with Power Platform.\n- **Ørsted**: Using Microsoft Intelligent Data Platform for predictive maintenance in wind energy.\n\n## Financial Highlights\n- Microsoft Cloud surpassed **$100 billion** in annualized revenue for the first time.\n- Notable revenue growth was reported across various segments:\n  - **Intelligent Cloud**: Increased by 25% to **$75.3 billion**.\n  - **Productivity and Business Processes**: Up 18% to **$63.4 billion**.\n  - **More Personal Computing**: Grew 10% to **$59.7 billion**.\n\n## Social Responsibility Initiatives\n- Focused on increasing access to digital skills and commitments to equip **10 million individuals** from underserved communities with job skills by 2025, especially in cybersecurity.\n- Provided **$3.2 billion** in technology donations to nonprofits, with plans to double outreach over the next five years.\n\n### Commitment to Inclusivity\n- Advocating for human rights and racial equity through community support initiatives.\n- Ensured that over **50 million** people in rural areas gained broadband access since 2017.\n\n### Sustainability Goals\n- Committed to becoming carbon negative by 2030, water positive by 2030, and zero waste by 2030, while achieving significant reductions in emissions.\n\n## Technology Innovations and Future Opportunities\nMicrosoft plans to leverage the increasing integration of technology in various sectors, anticipating significant growth in technology's contribution to global GDP.\n\n### Strategic Areas of Focus:\n- **Productivity and Business Processes**: Expanding tools like Microsoft 365, Dynamics 365, and Teams.\n- **Intelligent Cloud**: Succeeding through Azure and new AI services.\n- **Gaming**: Focusing on Xbox and cloud gaming solutions, with the acquisition of Activision Blizzard set to enhance offerings.\n\n## Company Culture and Values\nMicrosoft continues to prioritize a **growth mindset**, fostering a culture of inclusion, collaboration, and diversity to better serve its diverse global customer base.\n\n### Employee Engagement\n- Significant employee contributions with **$255 million** donated to nonprofits and over **720,000** volunteer hours recorded in 2022.\n\n## Conclusion\nChairman and CEO Satya Nadella expressed gratitude for shareholder support and emphasized that with continued commitment to innovation and social responsibility, Microsoft is poised for a transformative future.\n\n---\n\nMicrosoft's annual report emphasizes the company's resilience, its commitment to social responsibility, and potential for future growth, all while navigating complex global challenges."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "# replace with actual path to report file, after saving it to your drive\n",
        "\n",
        "content = f'''\n",
        "    Summarize the following annual report from Microsoft. Use\n",
        "    markdown formatting in your output:\n",
        "\n",
        "    {report}\n",
        "    '''\n",
        "\n",
        "messages = [{ 'role': 'user', 'content': content }]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "output = response.choices[0].message.content\n",
        "display(Markdown(output))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "codecamp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}