{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Vlad-Enia/NN-LLM-Intro/blob/master/Part%20II%20-%20LLMs/Demos/Text%20Generation/OpenAI_Text_Generation_Chat_Completions_API.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#OpenAI Account Setup\n"
      ],
      "metadata": {
        "id": "9WCNWXoo4y9A"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Creating OpenAI API key\n",
        "* First, [create](https://platform.openai.com/signup) an OpenAI account\n",
        "* Add [credit balance](https://platform.openai.com/settings/organization/billing/overview)\n",
        "  * Using the API is charged per token, each model having different prices\n",
        "  * We will mainly be using GPT-4o mini, which is the cost-effective model offered by OpenAI.\n",
        "  * For the purpose of these tutorials, $5 should be more than enough\n",
        "* Generate an [API key](https://platform.openai.com/api-keys)"
      ],
      "metadata": {
        "id": "E-Yf0KWf64qx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Storing and using the API key\n",
        "* Store the key as a secret in Google Colab\n",
        "  * Click the \"Key\" button on the left toolbar\n",
        "  * Add the API key as a secret with name `OPENAI_API_KEY`\n",
        "  * Enable `Notebook access` for the secret\n",
        "  * Access the key using:"
      ],
      "metadata": {
        "id": "mz8ZCDyE6guW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import userdata\n",
        "OPENAI_API_KEY = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "ih8q9PPo-ClM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HqaJxQ0r3VE8"
      },
      "source": [
        "# Generating text with Large Language Models (LLMs)\n",
        "\n",
        "`GPT-4o mini` is one a cost-effiecient model by [OpenAI](https://openai.com/). \" `GPT-4o Mini` can generate human-like prose by responding to prompts written in the [Chat Markup Language](https://learn.microsoft.com/en-us/azure/cognitive-services/openai/how-to/chatgpt), or ChatML for short. Here are few examples demonstrating how to leverage `GPT-4o mini`'s text-generation capabilities. Start by asking `GPT-4o mini` why is the sky blue. Run this cell several times and you'll get a different result each time. Set `temperature` to 0, however, and the results will be the same most of the time:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9l4rTRYF3VE9",
        "outputId": "a5d19649-35c6-4354-9d76-dbff72da1780",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sky appears blue primarily due to a phenomenon called Rayleigh scattering. When sunlight enters the Earth's atmosphere, it is made up of many colors, each corresponding to different wavelengths. Blue light has a shorter wavelength compared to other colors like red and yellow.\n",
            "\n",
            "As sunlight passes through the atmosphere, the shorter wavelengths of light (blue and violet) are scattered in all directions by the gases and particles in the air. Although violet light is scattered even more than blue light, our eyes are more sensitive to blue light and some of the violet light is absorbed by the ozone layer. As a result, we perceive the sky as blue.\n",
            "\n",
            "During sunrise and sunset, the sky can take on shades of red, orange, and pink because the sunlight has to pass through a greater thickness of the atmosphere. This causes more scattering of the shorter wavelengths and allows the longer wavelengths (reds and oranges) to dominate the colors we see.\n"
          ]
        }
      ],
      "source": [
        "from openai import OpenAI\n",
        "\n",
        "client = OpenAI(api_key=OPENAI_API_KEY)\n",
        "\n",
        "messages = [{\n",
        "    'role': 'user',\n",
        "    'content': 'Why is the sky blue?'\n",
        "}]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "print(response.choices[0].message.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NDr9ckH3VE9"
      },
      "source": [
        "You can richen the UI experience by streaming the response. Here's how:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BCnijYoM3VE9",
        "outputId": "d615b8b1-88e7-4578-a449-7a202854cf84",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sky appears blue primarily due to a phenomenon called Rayleigh scattering. This occurs when sunlight interacts with the Earth's atmosphere, which is made up of various gases and particles. \n",
            "\n",
            "Sunlight, or white light, consists of multiple colors, each with different wavelengths. Blue light has shorter wavelengths than red light. When sunlight passes through the atmosphere, the shorter blue wavelengths are scattered more than the longer wavelengths (like red) because they collide with air molecules more frequently.\n",
            "\n",
            "As a result, when you look up at the sky during the day, you see more of this scattered blue light. In contrast, during sunrise and sunset, the sunlight passes through a thicker layer of the atmosphere, scattering the shorter wavelengths out of the line of sight and allowing the longer wavelengths, such as reds and oranges, to dominate. This is why the sky can appear red or orange during those times."
          ]
        }
      ],
      "source": [
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7heTNb33VE9"
      },
      "source": [
        "## Context\n",
        "\n",
        "Messages transmitted to `GPT-4o-mini` use the Chat Markup Language. ChatML exists so that the context of a conversation can be preserved across calls. To demonstrate, ask the LLM what its name is:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I4-aVnsG3VE-",
        "outputId": "a6972362-6ca9-4d44-9a6d-a8d6c3253cde",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, Vlad! I'm an AI language model and don't have a personal name, but you can just call me Assistant. How can I help you today?"
          ]
        }
      ],
      "source": [
        "messages = [{\n",
        "    'role': 'user',\n",
        "    'content': 'My name is Vlad. What\\'s your name?' \\\n",
        "}]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eV229Fg3VE-"
      },
      "source": [
        "But now try this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t7zyuNjI3VE-",
        "outputId": "e5ceecbb-3997-4de5-d9d4-33e568046ecb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hello, Vlad! I’m called GPT. How can I assist you today?"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\n",
        "        'role': 'system',\n",
        "        'content': 'You are a friendly assistant named GPT.'\n",
        "    },\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': 'My name is Vlad. What\\'s your name?'\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J94DgGCE3VE-"
      },
      "source": [
        "You can be as specific as you’d like with `system` messages, even saying \"If you don’t know the answer to a question, say I don’t know.\" You can also prescribe a persona. Replace \"friendly\" with \"sarcastic\" in the message from system and run the code again. The response may be \"Oh, hi Jeff, I’m LISA. You can call me whatever you'd like, but don’t call me late for dinner.\" Run the code several times and there’s no end to the colorful responses you’ll receive.\n",
        "\n",
        "ChatML's greatest power lies in persisting context from one call to the next. As an example, try this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2hyXZVkC3VE-",
        "outputId": "c35605ef-4b1e-4e8e-ad48-229aa75b0df7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hi Vlad! I'm GPT, your friendly assistant. How can I help you today?"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\n",
        "        'role': 'system',\n",
        "        'content': 'You are a friendly assistant named GPT.'\n",
        "    },\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': 'My name is Vlad. What\\'s your name?'\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ppDNpc8S3VE-"
      },
      "source": [
        "Then follow up immediately with this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DoYOaZ2u3VE-",
        "outputId": "0153877e-b72b-45e7-a1be-c4abb5ca4cf7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I don't know your name unless you tell me. How can I assist you today?"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\n",
        "        'role': 'system',\n",
        "        'content': 'You are a friendly assistant named GPT.'\n",
        "    },\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': 'What is my name?'\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vysFP6vr3VE-"
      },
      "source": [
        "The LLM will respond with something along the lines of \"I'm sorry, but I don’t have access to that information.\" But now try this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gHXctS083VE-",
        "outputId": "894c0489-5ea9-4168-ad67-c9176965cc4f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Your name is Vlad!"
          ]
        }
      ],
      "source": [
        "messages = [\n",
        "    {\n",
        "        'role': 'system',\n",
        "        'content': 'You are a friendly assistant named GPT.'\n",
        "    },\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': 'My name is Vlad. What\\'s your name?'\n",
        "    },\n",
        "    {\n",
        "        'role': 'assistant',\n",
        "        'content': 'Hello Vlad, my name is GPT. Nice to meet you!'\n",
        "    },\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': 'What is my name?'\n",
        "    }\n",
        "]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8zGlnbsD3VE-"
      },
      "source": [
        "Get it? Calls to `GPT-4o mini` are stateless. If you give `GPT-4o mini` your name in one call and ask it to repeat your name in the next call, it has no clue. But with ChatML, you can provide past responses as context for the current call. You can build a conversational assistant simply by repeating the last few prompts and responses in each call to `GPT-4o mini`. The further back you go, the longer the assistant's \"memory\" will be."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cuk_lscY3VE-"
      },
      "source": [
        "## Tokenization\n",
        "\n",
        "LLMs don't work with words; they work with *tokens*. Tokenization plays an important role in Natural Language Processing. Neural networks can’t process text, at least not directly; they only process numbers. Tokenization converts words into numbers that a deep-learning model can understand. When an LLM generates a response by predicting a series of tokens, the tokenization process is reversed to convert the tokens into human-readable text.\n",
        "\n",
        "OpenAI LLMs use a form of tokenization called [Byte-Pair Encoding](https://en.wikipedia.org/wiki/Byte_pair_encoding) (BPE).\n",
        "\n",
        "\n",
        "As a rule of thumb, 3 words on average translate to about 4 BPE tokens. That’s important because LLMs limit the number of tokens in each API call. The maximum token count is controlled by a parameter named `max_tokens`. For `GPT-4o-mini`, which has a context window size of `128K` (i.e. you can input `128000` tokens), it's enough to pass in a document that's a few hundred pages long. If the number of tokens generated exceeds `max_tokens`, then either the call will fail or the response will be truncated.\n",
        "\n",
        "You can compute the number of tokens generated from a text sample with help from a Python package named [`tiktoken`](https://pypi.org/project/tiktoken/):"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-btL_Kx3E8mN",
        "outputId": "7a31220c-61e3-49d2-b72a-b7e655b48759"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2024.11.6)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken) (2025.1.31)\n",
            "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m32.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.9.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p8typHnn3VE-",
        "outputId": "fe4d14c8-ff70-4f7d-e9e9-0df6cef521e8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "96 tokens\n"
          ]
        }
      ],
      "source": [
        "import tiktoken\n",
        "\n",
        "text = '''\n",
        "    Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua.\n",
        "    Ut enim ad minim veniam, quis nostrud exercitation ullamco laboris nisi ut aliquip ex ea commodo consequat.\n",
        "    Duis aute irure dolor in reprehenderit in voluptate velit esse cillum dolore eu fugiat nulla pariatur.\n",
        "    Excepteur sint occaecat cupidatat non proident, sunt in culpa qui officia deserunt mollit anim id est laborum.\n",
        "    '''\n",
        "\n",
        "encoding = tiktoken.encoding_for_model('gpt-4o-mini')\n",
        "num_tokens = len(encoding.encode(text))\n",
        "print(f'{num_tokens} tokens')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yuLy40953VE_"
      },
      "source": [
        "You can learn more about tokens and how to estimate the number of tokens in a `messages` object using [OpenAI Cookbook](https://github.com/openai/openai-cookbook/blob/main/examples/How_to_count_tokens_with_tiktoken.ipynb)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xDzfVNH33VE_"
      },
      "source": [
        "There are a couple of reasons to be aware of the token count in each call. First, you’re charged by the token for input and output. The larger the `messages` array and the longer the response, the more you pay. Second, when using the messages array to provide context from previous calls, you have a finite amount of space to work with. It's common practice to pick a number – say, 10 or 20 – and limit the context from previous calls to that number of messages, or to programmatically compute the number of tokens that a conversation comprises and include as many messages as `max_tokens` will allow."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ts5z3Za33VE_"
      },
      "source": [
        "## Natural Language Processing\n",
        "\n",
        "`GPT-4o mini` can perform many NLP tasks such as sentiment analysis and neural machine translation (NMT) without further training. Here's an example that translates text from English to French. It's a good idea to set `temperature` to 0 here since you generally want translations to be accurate and repeatable rather than creative:"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Neural Machine Translation"
      ],
      "metadata": {
        "id": "WCYDM2TwLvz4"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qAuSht4S3VE_",
        "outputId": "987f9fb2-2309-4788-e8b7-e1e2c0a679e1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Brza smeđa lisica preskoči lenju psa."
          ]
        }
      ],
      "source": [
        "content =  f'Translate the following text from English to Serbian: The quick brown fox jumps over the lazy dog'\n",
        "\n",
        "messages = [{ 'role': 'user', 'content': content }]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    temperature=0,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Sentiment Analysis"
      ],
      "metadata": {
        "id": "-JzLTQZrL0t1"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IIe2hTyS3VE_"
      },
      "source": [
        "The following examples demonstrate how to use `GPT-4o-mini` for sentiment analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XRIl2gHJ3VE_",
        "outputId": "2320e853-dc89-4a4b-b00d-baa91f7dc3cf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The review's sentiment is positive."
          ]
        }
      ],
      "source": [
        "content = '''\n",
        "    Indicate whether the following review's sentiment is positive or\n",
        "    negative: Great food and excellent service.\n",
        "    '''\n",
        "\n",
        "messages = [{ 'role': 'user', 'content': content }]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LjWT0rTk3VE_",
        "outputId": "425120d5-11f1-4e17-c5af-e6152c8a219f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The sentiment of the review is negative."
          ]
        }
      ],
      "source": [
        "content = '''\n",
        "    Indicate whether the following review's sentiment is positive or\n",
        "    negative: Long lines and poor customer service.\n",
        "    '''\n",
        "\n",
        "messages = [{ 'role': 'user', 'content': content }]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Spam Filter"
      ],
      "metadata": {
        "id": "PSbevPSsL7l2"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0Figuk23VE_"
      },
      "source": [
        "Sentiment analysis is a text-classification task. LLMs can classify text in other ways, too. The next two examples demonstrate how to use `GPT-4o mini` as a spam filter:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "D6KdNoKT3VE_",
        "outputId": "6870ea9d-1e96-430e-a883-8773d85e59fa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Not spam."
          ]
        }
      ],
      "source": [
        "content = '''\n",
        "    Indicate whether the following email is spam or not spam:\n",
        "    Please plan to attend the code review at 2:00 p.m. this afternoon.\n",
        "    '''\n",
        "\n",
        "messages = [{ 'role': 'user', 'content': content }]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8ZBgSV2y3VE_",
        "outputId": "1541bb8e-64f4-4c47-bc97-027a25e6ec8d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The email is likely to be classified as spam. It promotes ordering prescription medications online, which is commonly associated with spam or potentially fraudulent offers, especially when it emphasizes saving money in an unsolicited manner."
          ]
        }
      ],
      "source": [
        "content = '''\n",
        "    Indicate whether the following email is spam or not spam:\n",
        "    Order prescription meds online and save $$$.\n",
        "    '''\n",
        "\n",
        "messages = [{ 'role': 'user', 'content': content }]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content = '''\n",
        "    Indicate whether the following email is spam or not spam:\n",
        "    Order prescription meds online and save $$$.\n",
        "    '''\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "      'role': 'system',\n",
        "      'content':\n",
        "        '''You are a spam filter. Please evaluate using \\\"Spam\\\" or \\\"Not Spam\".\n",
        "          If you cannot evaluate, please respond with \\\"Cannot evaluate\\\"'''\n",
        "    },\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': content\n",
        "    }\n",
        "  ]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GI8CFaIoIlXF",
        "outputId": "578201ce-74ca-4be0-ae16-d00b7e1e575f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Spam"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "content = '''\n",
        "    12341241241\n",
        "    '''\n",
        "\n",
        "messages = [\n",
        "    {\n",
        "      'role': 'system',\n",
        "      'content':\n",
        "        '''You are a spam filter. Please evaluate using \\\"Spam\\\" or \\\"Not Spam\".\n",
        "          If you cannot evaluate, please respond with \\\"Cannot evaluate\\\"'''\n",
        "    },\n",
        "    {\n",
        "        'role': 'user',\n",
        "        'content': content\n",
        "    }\n",
        "  ]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages,\n",
        "    stream=True\n",
        ")\n",
        "\n",
        "for chunk in response:\n",
        "    content = chunk.choices[0].delta.content\n",
        "    if content is not None:\n",
        "        print(content, end='')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y5NjKNGOJfU7",
        "outputId": "0e0f4b84-75e6-4283-bd11-5c7ce774c917"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cannot evaluate"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Parsing Raw Data"
      ],
      "metadata": {
        "id": "7-aK6nk5MBE3"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJN58wlf3VE_"
      },
      "source": [
        "A practical use for LLMs is parsing freeform address fields and generating structured data. Here's an example:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cd2iYo-73VE_",
        "outputId": "8a3651fb-51f9-4597-a5c6-c6b7b3f70253",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{\n",
            "    \"Name\": \"Recipient\",\n",
            "    \"Street Address\": \"11 Aviation Avenue\",\n",
            "    \"City\": \"Charlottetown\",\n",
            "    \"State\": \"Prince Edward Island\",\n",
            "    \"Country\": \"Canada\",\n",
            "    \"Postal Code\": \"C1E 0A1\"\n",
            "}\n",
            "{\n",
            "    \"Name\": \"Roche Molecular Systems, Inc.\",\n",
            "    \"Street Address\": \"4300 Hacienda Drive\",\n",
            "    \"City\": \"Pleasanton\",\n",
            "    \"State\": \"California\",\n",
            "    \"Country\": \"United States\",\n",
            "    \"Postal Code\": \"94588\"\n",
            "}\n",
            "{\n",
            "    \"Name\": \"Cross Research S.A.\",\n",
            "    \"Street Address\": \"Via F.A. Giorgioli 14\",\n",
            "    \"City\": \"Arzo\",\n",
            "    \"State\": \"\",\n",
            "    \"Country\": \"Switzerland\",\n",
            "    \"Postal Code\": \"6864\"\n",
            "}\n",
            "{\n",
            "    \"Name\": \"Wasdell Group\",\n",
            "    \"Street Address\": \"Wasdell Packaging Ltd Unit 1-8, Euroway Industrial Estate, Blagrove\",\n",
            "    \"City\": \"Swindon\",\n",
            "    \"State\": \"\",\n",
            "    \"Country\": \"United Kingdom\",\n",
            "    \"Postal Code\": \"SN5 8YW\"\n",
            "}\n",
            "{\n",
            "    \"Name\": \"Recipient\",\n",
            "    \"Street Address\": \"Policlinico Gemelli, 4th Floor, Wing J, Largo Gemelli 8\",\n",
            "    \"City\": \"Rome\",\n",
            "    \"State\": \"\",\n",
            "    \"Country\": \"Italy\",\n",
            "    \"Postal Code\": \"00168\"\n",
            "}\n",
            "{\n",
            "    \"Name\": \"Academisch Ziekenhuis Maastricht\",\n",
            "    \"Street Address\": \"P. Debyelaan 25 5e\",\n",
            "    \"City\": \"Maastricht\",\n",
            "    \"State\": \"\",\n",
            "    \"Country\": \"Netherlands\",\n",
            "    \"Postal Code\": \"6229 HX\"\n",
            "}\n",
            "{\n",
            "    \"Name\": \"Wintellect Brussels\",\n",
            "    \"Street Address\": \"Leuvensesteenweg 555\",\n",
            "    \"City\": \"Marken Benelux\",\n",
            "    \"State\": \"\",\n",
            "    \"Country\": \"Belgium\",\n",
            "    \"Postal Code\": \"1930\"\n",
            "}\n",
            "{\n",
            "    \"Name\": \"SCM department, AstraZeneca K.K.\",\n",
            "    \"Street Address\": \"Maihara Factory, AstraZeneca K.K., 215-31\",\n",
            "    \"City\": \"Miyoshi\",\n",
            "    \"State\": \"Shiga\",\n",
            "    \"Country\": \"Japan\",\n",
            "    \"Postal Code\": \"215-31\"\n",
            "}\n",
            "{\n",
            "    \"Name\": \"Healthcare Logistics Australia\",\n",
            "    \"Street Address\": \"7 Dolerite Way\",\n",
            "    \"City\": \"Pemulwuy\",\n",
            "    \"State\": \"New South Wales\",\n",
            "    \"Country\": \"Australia\",\n",
            "    \"Postal Code\": \"2145\"\n",
            "}\n",
            "{\n",
            "    \"Name\": \"Suncoast Research\",\n",
            "    \"Street Address\": \"2128 W Flagler St, Suite 101\",\n",
            "    \"City\": \"Miami\",\n",
            "    \"State\": \"Florida\",\n",
            "    \"Country\": \"United States\",\n",
            "    \"Postal Code\": \"33135\"\n",
            "}\n"
          ]
        }
      ],
      "source": [
        "addresses = [\n",
        "    '11 Aviation Avenue, Charlottetown, PE C1E 0A1, Canada',\n",
        "    'Roche Molecular Systems, Inc., 4300 Hacienda Drive, Pleasanton, CA 94588, US',\n",
        "    'Cross Research S.A., Phase I Unit, Via F.A. Giorgioli 14, Arzo, 6864, CH',\n",
        "    'Wasdell Group, Wasdell Packaging Ltd Unit 1-8, Euroway Industrial Estate, Blagrove, Swindon, SN5 8YW, GB',\n",
        "    'Policlinico Gemelli, 4th Floor, Wing J, Largo Gemelli 8, Rome, 00168, IT',\n",
        "    'Academisch Ziekenhuis Maastricht, CDL Stamcellaboratorium, P. Debyelaan 25 5e, Maastricht, 6229 HX, NL',\n",
        "    'Wintellect Brussels, Leuvensesteenweg 555, Marken Benelux, 1930, BE',\n",
        "    'SCM department, AstraZeneca K.K., Maihara Factory, AstraZeneca K.K., 215-31, Miyos, Shiga-Ken, 215-31, JP',\n",
        "    'Healthcare Logistics Australia, 7 Dolerite Way, Pemulwuy NSW 2145, AU',\n",
        "    'Suncoast Research, 2128 W Flagler St, Suite 101, Mami, FL, 33135, US'\n",
        "]\n",
        "\n",
        "for address in addresses:\n",
        "    content = f'''\n",
        "        Parse the freeform address below into fields and return a JSON\n",
        "        representation that uses the following format. Convert country\n",
        "        abbreviations such as \"CA\" into country names such as \"Canada\"\n",
        "        and state abbreviations such as \"CA\" into state names such as\n",
        "        \"California.\" Leave unknown fields blank. Also correct any\n",
        "        obvious misspellings.\n",
        "\n",
        "        {{\n",
        "            \"Name\": \"Recipient\",\n",
        "            \"Street Address\": \"Street address\",\n",
        "            \"City\": \"City, town, etc.\",\n",
        "            \"State\": \"State, province, region, territory, canton, county, department, länder, or prefecture\",\n",
        "            \"Country\": \"Country name\",\n",
        "            \"Postal Code\": \"Postal code\"\n",
        "        }}\n",
        "\n",
        "        Address: {address}\n",
        "        '''\n",
        "\n",
        "    messages = [{ 'role': 'user', 'content': content }]\n",
        "\n",
        "    response = client.chat.completions.create(\n",
        "        model='gpt-4o-mini',\n",
        "        messages=messages,\n",
        "        response_format={ 'type': 'json_object' },\n",
        "        stream=True\n",
        "    )\n",
        "\n",
        "    for chunk in response:\n",
        "        content = chunk.choices[0].delta.content\n",
        "        if content is not None:\n",
        "            print(content, end='')\n",
        "\n",
        "    print()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Ingesting Whole Documents"
      ],
      "metadata": {
        "id": "se27LKGNMKGe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbU0cLT-3VE_"
      },
      "source": [
        "`GPT-4o-mini`'s 128K context-window size enables it to ingest large documents for summarization or other purposes. Let's see what it can do with Microsoft's 2022 annual report. We will ask it format the output using Markdown, and then use the `IPython.display` package to properly display the generated summary."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "B22wV5VcDID7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20bd740b-f47a-4040-8228-4b9f791b4b65"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cajZOHWN3VE_",
        "outputId": "94dab03c-2c01-4c89-cc8a-4440a41c2b23",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Markdown object>"
            ],
            "text/markdown": "# Microsoft Annual Report Summary (Fiscal Year 2022)\n\n## Overview\nMicrosoft's mission remains to empower every person and organization on the planet to achieve more, amidst significant societal, economic, and geopolitical changes. The year 2022 presented challenges like high inflation and stretched supply chains but also opportunities for technological advancement. Microsoft proactively supports digital transformation across various industries.\n\n## Key Highlights\n- **Record Financial Performance**: Microsoft reported $198 billion in revenue and $83 billion in operating income, with the Microsoft Cloud surpassing $100 billion in annualized revenue for the first time.\n- **Customer Examples**: \n  - **Ferrovial** used Microsoft Cloud for safer autonomous road infrastructure.\n  - **Peace Parks Foundation** leveraged Microsoft Dynamics 365 for wildlife protection.\n  - **Kawasaki Heavy Industries** is creating an industrial metaverse using Azure IoT and HoloLens.\n  - **Globo** utilized Power Platform for employee-driven solution creation.\n  - **Ørsted** tapped the Microsoft Intelligent Data Platform for predictive maintenance in wind energy.\n\n## Responsibilities & Commitments\nMicrosoft emphasizes the intersection of opportunity and responsibility, with initiatives centered on:\n- **Inclusive Economic Growth**: Providing digital skills training to over 23 million people, with plans to equip 10 million underserved individuals for jobs in the digital economy by 2025.\n- **Protection of Rights**: Commitment to support fundamental rights, racial equity, and access to technology and education.\n- **Sustainability Efforts**: Ambitions to become carbon negative, water positive, and achieve zero waste by 2030.\n- **Building Trust**: Ensuring customer data privacy and robust cybersecurity measures, with major investments to protect users.\n\n## Financial Breakdown\n- **Segment Revenue**:\n  - **Productivity and Business Processes**: $63.4 billion (18% increase)\n  - **Intelligent Cloud**: $75.3 billion (25% increase)\n  - **More Personal Computing**: $59.7 billion (10% increase)\n\n- **Operating Income**:\n  - **Productivity and Business Processes**: $29.7 billion\n  - **Intelligent Cloud**: $32.7 billion\n  - **More Personal Computing**: $21.0 billion\n\n## Key Investments & Initiatives\n- **Acquisitions**: \n  - **Nuance Communications** acquisition to enhance AI-driven healthcare solutions (acquired for $18.8 billion).\n  - **ZeniMax Media** acquisition for expanding gaming offerings (completed for $8.1 billion).\n  - **Activision Blizzard** acquisition, valued at $68.7 billion, pending regulatory approval.\n\n- **Stock Repurchases & Dividends**: Microsoft repurchased 95 million shares for $28 billion and distributed $2.48 per share in dividends.\n\n## Cultural Commitment\nMicrosoft emphasizes a growth mindset, focusing on diversity and inclusion, evident from improved representation across various demographics. Employee engagement in charitable efforts exceeded $255 million.\n\n## Future Outlook\nAs Microsoft adapts to the increasingly digital imperative in the economy, it aims to further empower organizations and individuals through technology across all sectors. The company expects technology's GDP share to rise from 5% to 10%, positioning itself at the forefront of this transition.\n\n---\n\nThis summary captures the essence and highlights of Microsoft's FY 2022 Annual Report, showcasing its strategic focus areas, financial achievements, and commitments to responsibility and sustainability."
          },
          "metadata": {}
        }
      ],
      "source": [
        "from IPython.display import Markdown, display\n",
        "\n",
        "MSFT_ANNUAL_REPORT_FILE_PATH = userdata.get('MSFT_ANNUAL_REPORT_FILE_PATH')\n",
        "\n",
        "with open(MSFT_ANNUAL_REPORT_FILE_PATH, 'r') as file:\n",
        "    report = file.read()\n",
        "\n",
        "content = f'''\n",
        "    Summarize the following annual report from Microsoft. Use\n",
        "    markdown formatting in your output:\n",
        "\n",
        "    {report}\n",
        "    '''\n",
        "\n",
        "messages = [{ 'role': 'user', 'content': content }]\n",
        "\n",
        "response = client.chat.completions.create(\n",
        "    model='gpt-4o-mini',\n",
        "    messages=messages\n",
        ")\n",
        "\n",
        "output = response.choices[0].message.content\n",
        "display(Markdown(output))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "codecamp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.2"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}